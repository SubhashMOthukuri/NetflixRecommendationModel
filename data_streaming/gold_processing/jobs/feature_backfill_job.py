"""
Feature Backfill Job - Gold Layer
Computes historical features for training data with point-in-time correctness (time travel)
Enables correct training data generation without data leakage
"""
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, lit
from libs.logger import get_logger
from libs.exceptions import PipelineException

# Core infrastructure
from data_streaming.gold_processing.core.silver_reader import SilverReader
from data_streaming.gold_processing.core.feast_client import FeastClient

# Feature engineering
from data_streaming.gold_processing.feature_engineering.user_features import UserFeatures
from data_streaming.gold_processing.feature_engineering.item_features import ItemFeatures
from data_streaming.gold_processing.feature_engineering.session_features import SessionFeatures
from data_streaming.gold_processing.feature_engineering.statistical_features import StatisticalFeatures
from data_streaming.gold_processing.feature_engineering.temporal_features import TemporalFeatures

# Storage
from data_streaming.gold_processing.storage.gold_writer import GoldWriter

# Monitoring
from data_streaming.gold_processing.monitoring.gold_metrics import GoldMetrics

# Spark session
from data_streaming.bronze_ingestion.core.spark.spark_session import get_spark_session

logger = get_logger(__name__)


class FeatureBackfillJobError(PipelineException):
    """Raised when feature backfill job fails"""
    pass


class FeatureBackfillJob:
    """
    Production-grade feature backfill job for training data generation
    
    Computes historical features with point-in-time correctness:
    - Features computed as of event timestamp (no future data leakage)
    - Supports time travel queries
    - Generates training datasets with correct features
    
    Example:
        job = FeatureBackfillJob()
        job.backfill_features(
            start_date="2024-01-01",
            end_date="2024-01-31",
            entity_ids=["user_123", "user_456"]
        )
    """
    
    def __init__(
        self,
        spark: Optional[SparkSession] = None,
        environment: Optional[str] = None
    ):
        """
        Initialize feature backfill job
        
        Args:
            spark: Optional SparkSession (creates new if None)
            environment: Environment name (dev/staging/prod)
        """
        self.environment = environment
        
        # Initialize Spark
        if spark is None:
            logger.info("Initializing Spark session...")
            self.spark = get_spark_session(environment=environment, app_name="feature_backfill_job")
        else:
            self.spark = spark
        
        # Initialize components
        logger.info("Initializing backfill components...")
        self.silver_reader = SilverReader(self.spark)
        self.feast_client = FeastClient()
        
        # Feature engineers
        self.user_features = UserFeatures(self.spark)
        self.item_features = ItemFeatures(self.spark)
        self.session_features = SessionFeatures(self.spark)
        self.statistical_features = StatisticalFeatures(self.spark)
        self.temporal_features = TemporalFeatures(self.spark)
        
        # Storage
        self.gold_writer = GoldWriter(self.spark)
        
        # Monitoring
        self.metrics = GoldMetrics()
        
        logger.info("Feature backfill job initialized")
    
    def backfill_features(
        self,
        start_date: str,
        end_date: str,
        entity_ids: Optional[List[str]] = None,
        entity_type: str = "user",
        feature_types: Optional[List[str]] = None,
        output_path: Optional[str] = None
    ) -> DataFrame:
        """
        Backfill features for historical date range with point-in-time correctness
        
        Args:
            start_date: Start date (YYYY-MM-DD)
            end_date: End date (YYYY-MM-DD)
            entity_ids: Optional list of entity IDs to backfill (if None, backfills all)
            entity_type: Type of entity ("user", "item", "session")
            feature_types: Optional list of feature types to compute (defaults to all)
            output_path: Optional output path for training dataset
        
        Returns:
            DataFrame with backfilled features
        
        Example:
            # Backfill user features for January 2024
            features_df = job.backfill_features(
                start_date="2024-01-01",
                end_date="2024-01-31",
                entity_type="user"
            )
        """
        try:
            logger.info("=" * 80)
            logger.info("Starting Feature Backfill Job")
            logger.info(f"Date range: {start_date} to {end_date}")
            logger.info(f"Entity type: {entity_type}")
            logger.info("=" * 80)
            
            start_time = datetime.now()
            
            # Step 1: Read silver data for date range
            logger.info("Step 1: Reading silver data for backfill...")
            silver_df = self.silver_reader.read_silver(
                start_date=start_date,
                end_date=end_date
            )
            
            # Filter by entity IDs if provided
            if entity_ids:
                if entity_type == "user":
                    silver_df = silver_df.filter(silver_df.user_id.isin(entity_ids))
                elif entity_type == "item":
                    silver_df = silver_df.filter(silver_df.video_id.isin(entity_ids))
                elif entity_type == "session":
                    silver_df = silver_df.filter(silver_df.session_id.isin(entity_ids))
                logger.info(f"Filtered to {len(entity_ids)} entities")
            
            silver_count = silver_df.count()
            logger.info(f"Read {silver_count:,} records for backfill")
            
            if silver_count == 0:
                logger.warning("No data to backfill, exiting")
                return self.spark.createDataFrame([], schema="entity_id STRING")
            
            # Step 2: Generate point-in-time timestamps for each event
            logger.info("Step 2: Generating point-in-time timestamps...")
            # Use event_timestamp as point-in-time for each record
            silver_df = silver_df.withColumn(
                "point_in_time",
                col("event_timestamp")
            )
            
            # Step 3: Compute features with point-in-time correctness
            logger.info("Step 3: Computing features with point-in-time correctness...")
            features_list = []
            
            # Default feature types
            if feature_types is None:
                feature_types = ["user", "item", "session", "temporal"]
            
            # Group by entity and compute features for each point-in-time
            if entity_type == "user" and "user" in feature_types:
                logger.info("Computing user features with time travel...")
                # For each unique user and timestamp, compute features up to that point
                user_ids = silver_df.select("user_id").distinct().collect()
                user_id_list = [row.user_id for row in user_ids]
                
                for user_id in user_id_list[:100]:  # Limit for demo (in production, batch process)
                    user_events = silver_df.filter(col("user_id") == user_id)
                    
                    # Get unique timestamps for this user
                    timestamps = user_events.select("event_timestamp").distinct().orderBy("event_timestamp").collect()
                    
                    for timestamp_row in timestamps:
                        point_in_time = timestamp_row.event_timestamp
                        point_in_time_str = datetime.fromtimestamp(point_in_time / 1000).strftime("%Y-%m-%d %H:%M:%S")
                        
                        # Compute features up to this point in time
                        user_features = self.user_features.compute_features(
                            user_events,
                            point_in_time=point_in_time_str
                        )
                        
                        # Add point-in-time timestamp
                        user_features = user_features.withColumn(
                            "point_in_time",
                            lit(point_in_time)
                        )
                        
                        features_list.append(user_features)
                
                logger.info(f"Computed user features for {len(user_id_list)} users")
            
            elif entity_type == "item" and "item" in feature_types:
                logger.info("Computing item features with time travel...")
                item_features = self.item_features.compute_features(
                    silver_df,
                    point_in_time=None  # Will use event_timestamp from silver_df
                )
                features_list.append(item_features)
            
            elif entity_type == "session" and "session" in feature_types:
                logger.info("Computing session features with time travel...")
                session_features = self.session_features.compute_features(
                    silver_df,
                    point_in_time=None
                )
                features_list.append(session_features)
            
            # Combine all features
            if features_list:
                logger.info("Combining features...")
                from pyspark.sql.functions import col
                
                # Union all feature DataFrames
                combined_features = features_list[0]
                for feature_df in features_list[1:]:
                    # Align schemas before union
                    combined_features = combined_features.unionByName(feature_df, allowMissingColumns=True)
                
                backfilled_features = combined_features
            else:
                logger.warning("No features computed")
                backfilled_features = self.spark.createDataFrame([], schema="entity_id STRING")
            
            # Step 4: Add temporal features if requested
            if "temporal" in feature_types:
                logger.info("Adding temporal features...")
                temporal_features = self.temporal_features.compute_features(
                    silver_df,
                    point_in_time=None
                )
                
                # Merge with backfilled features
                if entity_type == "user":
                    backfilled_features = backfilled_features.join(
                        temporal_features.select("user_id", "days_since_last_event", "events_per_day"),
                        on="user_id",
                        how="left"
                    )
            
            # Step 5: Write backfilled features
            if output_path:
                logger.info(f"Step 5: Writing backfilled features to {output_path}...")
                backfilled_features.write \
                    .format("parquet") \
                    .option("compression", "snappy") \
                    .mode("overwrite") \
                    .save(output_path)
                logger.info(f"Backfilled features written to {output_path}")
            else:
                logger.info("Step 5: Skipping write (no output_path provided)")
            
            # Metrics
            total_duration = (datetime.now() - start_time).total_seconds() * 1000
            feature_count = backfilled_features.count()
            
            self.metrics.record_batch_processed(input_count=silver_count, output_count=feature_count)
            self.metrics.record_latency(total_duration, operation="feature_backfill")
            
            logger.info("=" * 80)
            logger.info("Feature Backfill Job Complete")
            logger.info(f"Total duration: {total_duration:.2f}ms")
            logger.info(f"Input: {silver_count:,} records â†’ Output: {feature_count:,} feature records")
            logger.info("=" * 80)
            
            return backfilled_features
            
        except Exception as e:
            logger.error(f"Feature backfill job failed: {str(e)}", exc_info=True)
            self.metrics.record_error("backfill_error", str(e))
            raise FeatureBackfillJobError(f"Cannot run feature backfill job: {str(e)}")
    
    def backfill_training_dataset(
        self,
        events_df: DataFrame,
        feature_types: Optional[List[str]] = None
    ) -> DataFrame:
        """
        Generate training dataset with point-in-time features
        
        Args:
            events_df: DataFrame with events (must have event_timestamp and entity_id columns)
            feature_types: Optional list of feature types to compute
        
        Returns:
            DataFrame with events joined to point-in-time features
        
        Example:
            # Generate training dataset for model training
            training_df = job.backfill_training_dataset(
                events_df=labeled_events,
                feature_types=["user", "item", "temporal"]
            )
        """
        try:
            logger.info("Generating training dataset with point-in-time features...")
            
            # Extract entity IDs and timestamps
            entity_ids = events_df.select("user_id").distinct().collect()
            entity_id_list = [row.user_id for row in entity_ids]
            
            # Get date range from events
            from pyspark.sql.functions import min as spark_min, max as spark_max
            date_range = events_df.agg(
                spark_min("event_date").alias("min_date"),
                spark_max("event_date").alias("max_date")
            ).collect()[0]
            
            start_date = date_range["min_date"]
            end_date = date_range["max_date"]
            
            logger.info(f"Date range: {start_date} to {end_date}")
            
            # Backfill features for this date range
            features_df = self.backfill_features(
                start_date=str(start_date),
                end_date=str(end_date),
                entity_ids=entity_id_list,
                entity_type="user",
                feature_types=feature_types
            )
            
            # Join features to events (point-in-time join)
            logger.info("Joining features to events (point-in-time)...")
            training_df = events_df.join(
                features_df,
                on="user_id",
                how="left"
            )
            
            logger.info(f"Training dataset generated: {training_df.count():,} records")
            
            return training_df
            
        except Exception as e:
            logger.error(f"Failed to generate training dataset: {str(e)}", exc_info=True)
            raise FeatureBackfillJobError(f"Cannot generate training dataset: {str(e)}")
    
    def backfill_incremental(
        self,
        last_backfill_date: str,
        current_date: Optional[str] = None,
        entity_type: str = "user"
    ):
        """
        Incremental backfill (backfill only new data since last backfill)
        
        Args:
            last_backfill_date: Last backfill date (YYYY-MM-DD)
            current_date: Current date (defaults to today)
            entity_type: Type of entity
        
        Example:
            job.backfill_incremental(
                last_backfill_date="2024-01-15",
                entity_type="user"
            )
        """
        try:
            if current_date is None:
                current_date = datetime.now().strftime("%Y-%m-%d")
            
            logger.info(f"Incremental backfill: {last_backfill_date} to {current_date}")
            
            # Backfill for date range
            return self.backfill_features(
                start_date=last_backfill_date,
                end_date=current_date,
                entity_type=entity_type
            )
            
        except Exception as e:
            logger.error(f"Incremental backfill failed: {str(e)}", exc_info=True)
            raise FeatureBackfillJobError(f"Cannot run incremental backfill: {str(e)}")

